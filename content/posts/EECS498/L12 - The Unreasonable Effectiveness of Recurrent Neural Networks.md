---
title: "L12 - The Unreasonable Effectiveness of Recurrent Neural Networks"
date: "2025-03-20 09:49:07"
author: "EvanZhangcn"
draft: false
categories: ["EECS498"]  # 在此编辑分类
tags: []               # 在此添加标签
weight: 12
url: "/posts/EECS498/L12 - The Unreasonable Effectiveness of Recurrent Neural Networks"  # 自动生成的URL
---
https://karpathy.github.io/2015/05/21/rnn-effectiveness/

**Sequences**

### **1. 普通神经网络和卷积神经网络的局限性**

- **固定大小的输入和输出**：普通神经网络和卷积神经网络的输入和输出都是固定大小的向量。例如，输入可能是一张固定大小的图像，输出可能是不同类别的概率向量。
- **固定的计算步骤**：
  这些模型通过a fixed amount of computational steps（例如固定的层数）来完成输入到输出的映射。

### **2. 循环神经网络（RNN）的优势**

- **处理序列数据**：RNN的核心优势在于它可以处理**序列数据**，即输入和输出可以是**向量序列**。例如，输入可以是一个时间序列（如语音信号），输出也可以是另一个序列（如翻译结果）。
- **灵活性**：
  RNN不仅可以在输入和输出中处理序列，还可以在输入和输出之间建立复杂的映射关系。这种灵活性使得RNN在处理动态数据（如文本、语音、视频）时非常强大。

从程序的角度理解RNN的工作原理：

### **1. RNN的工作原理**

- **状态向量与输入向量的结合**：RNN通过一个固定的（但通过学习得到的）函数，将输入向量与当前的状态向量结合起来，生成一个新的状态向量。这个过程可以理解为在编程中运行一个固定程序，输入某些数据并更新内部变量。
- **RNN的本质**：
  从这个角度看，RNN本质上是在描述程序。它通过不断更新状态向量来处理序列数据，类似于程序在执行过程中不断更新内部状态。

### **2. RNN的图灵完备性**

- **图灵完备性**：RNN是**图灵完备的**，这意味着在理论上，RNN可以模拟任意程序（只要权重设置得当）。这与神经网络的通用逼近定理类似，表明RNN具有极高的表达能力。
- **实际意义**：
  **尽管RNN具有图灵完备性，但这并不意味着它在实际应用中能够轻松解决所有问题。** 理论上的能力并不直接转化为实际性能，因此不必过度解读这一点。

### **3. 训练RNN的本质**

- **普通神经网络的训练**：训练普通神经网络可以看作是对函数的优化，**即通过调整参数来最小化损失函数。**
- **RNN的训练**：
  训练RNN则是对程序的优化，因为RNN通过状态向量的更新来描述动态的计算过程。**优化RNN的过程实际上是在优化一个能够处理序列数据的程序。**

一个**普通循环神经网络（Vanilla RNN）的前向传播过程**。

### **1. 隐藏状态的更新**

RNN的核心是**通过当前输入和前一时刻的隐藏状态**来更新当前时刻的隐藏状态。公式如下：

$$
h_t = \tanh(W_{hh} \cdot h_{t-1} + W_{xh} \cdot x_t)
$$

- $h_t$：当前时刻的隐藏状态。
- $h_{t-1}$：前一时刻的隐藏状态。
- $x_t$：当前时刻的输入。
- $W_{hh}$：隐藏状态到隐藏状态的权重矩阵。
- $W_{xh}$：输入到隐藏状态的权重矩阵。
- $\tanh$：非线性激活函数，将值压缩到 $[-1, 1]$ 范围内。

### 2.**RNN前向传播两个主要步骤*

RNN的前向传播过程包括两个主要步骤：

1. **隐藏状态更新**：通过前一时刻的隐藏状态和当前输入计算当前时刻的隐藏状态，where tanh is applied elementwise. 公式为：
   $$
   h_t = \tanh(W_{hh} \cdot h_{t-1} + W_{xh} \cdot x_t)
   $$
2. **输出计算**：通过当前时刻的隐藏状态计算输出，公式为：
   $$
   y_t = W_{hy} \cdot h_t
   $$

这个过程通过矩阵乘法和非线性激活函数 $\tanh$ 实现，是RNN处理序列数据的基础。

RNN的输入，输出具体是在做什么？**如何使用循环神经网络（RNN）进行文本生成**。

1. **任务目标**：给RNN输入一大段文本，让它学习**序列中下一个字符的概率分布**，即在给定前面字符序列的情况下，预测下一个字符的可能性。
   We’ll give the RNN a huge chunk of text and ask it to **model the probability distribution of the next character** in the sequence given a sequence of previous characters.
2. **生成文本**：
   一旦RNN学会了这种概率分布，就可以通过**逐个字符生成**的方式，生成新的文本。
   This will then allow us to generate new text one character at a time.

**如何通过反向传播算法训练循环神经网络（RNN）来预测字符序列**？
这段话详细解释了**如何通过反向传播算法训练循环神经网络（RNN）来预测字符序列**。以下是逐步解释：

---

### **1. RNN的预测过程**

- **输入字符**：在第一个时间步，RNN看到字符“h”，并预测下一个字符的概率分布。例如：

  - “h” 的置信度为 1.0
  - “e” 的置信度为 2.2
  - “l” 的置信度为 -3.0
  - “o” 的置信度为 4.1
- **目标字符**：
  在训练数据中（例如字符串“hello”），下一个正确的字符是“e”。因此，我们希望RNN增加对“e”的置信度（绿色），并减少对其他字符的置信度（红色）。

### **2. 反向传播算法**

- **可微操作**：RNN的所有操作都是可微的，**RNN的所有操作都是可微的**，是因为RNN的前向传播过程由一系列**可微分的数学运算**组成。这些运算包括矩阵乘法、向量加法以及非线性激活函数（如  或 ），它们都是可微分的。因此可以使用**反向传播算法**来调整网络权重。
- **梯度方向**：通过反向传播，计算每个权重的梯度，确定如何调整权重才能增加正确目标字符的置信度（绿色数字）。
- **参数更新**：
  根据梯度方向，对每个权重进行微调（参数更新）。这种更新会使得正确字符的置信度略微增加，错误字符的置信度略微降低。

### **3. 迭代训练**

- **重复过程**：将上述过程重复多次，每次输入相同的字符序列，RNN会逐步调整权重，使得正确字符的置信度越来越高，错误字符的置信度越来越低。
- **收敛**：
  最终，RNN会收敛到一种状态，其预测结果与训练数据一致，即总是能正确预测下一个字符。

### **4. 示例**

- **初始预测**：在第一个时间步，RNN对“e”的置信度为 2.2。
- **参数更新后**：经过一次参数更新后，RNN对“e”的置信度可能提高到 2.3，而对其他字符的置信度会降低。
- **最终结果**：
  经过多次训练后，RNN会稳定地预测“e”为下一个字符。
